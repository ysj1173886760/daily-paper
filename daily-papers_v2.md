# 2503.04388v1

['\n\n```markdown\n# 论文总结：More Documents, Same Length: Isolating the Challenge of Multiple Documents in RAG\n\n## 作者\nShahar Levy*, Nir Mazor*, Lihi Shalmon*, Michael Hassid, Gabriel Stanovsky  \n*希伯来大学计算机科学与工程学院*\n\n## 核心研究问题\n**在固定输入长度下，检索文档数量的增加如何影响LLM的性能？**  \n（分离长上下文处理与多文档处理的挑战）\n\n---\n\n## 关键发现\n1. **多文档处理是独立挑战**  \n   - 在相同token数量下，文档数量增加会导致LLM性能下降（最高达10%）。\n   - 与长上下文处理不同，多文档需处理冗余、冲突信息和隐含跨文档关系。\n\n2. **模型表现差异**  \n   - **Llama-3.1** 和 **Gemma-2** 性能显著下降，**Qwen-2** 表现稳定（可能更擅长多文档处理）。\n   - 小模型（7-9B参数）趋势类似，但影响较弱。\n\n3. **干扰文档的影响**  \n   - **相关但无答案的文档**（MuSiQue原始干扰文档）会混淆模型。\n   - **随机干扰文档**（无关文本）反而提升性能（可能提供额外上下文线索）。\n\n---\n\n## 方法论\n### 数据集构建\n- **基础数据**：基于多跳QA数据集MuSiQue（2,417个问题，每个问题关联20个文档，其中2-4个为支持文档）。\n- **控制变量**：\n  - 逐步减少文档数（20 → 15 → 10 → 8 → 支持文档数）。\n  - 保持总token数不变：删除干扰文档时，扩展剩余文档内容（从原Wikipedia文章追加文本）。\n  - 确保支持文档的关键信息位置不变。\n\n### 实验设置\n- **评估模型**：Llama-3.1 (8B/70B), Qwen2 (7B/72B), Gemma2 (9B/27B)\n- **指标**：答案与标准答案的F1重叠分数（基于MuSiQue标准）\n- **对比实验**：\n  - **仅支持文档**（短上下文）→ 性能显著提升\n  - **无文档输入** → F1≈0.02（排除数据污染影响）\n\n---\n\n## 主要结果\n![图2：文档数量对性能的影响](https://i.imgur.com/placeholder.png)  \n*注：横坐标为文档数，纵坐标为F1分数*\n\n| 模型               | 20文档 | 8文档 | 仅支持文档 |\n|--------------------|--------|-------|------------|\n| Llama-3.1-70B     | 0.44   | 0.52  | 0.61       |\n| Gemma-2-27B       | 0.52   | 0.58  | 0.65       |\n| Qwen-2-72B        | 0.60   | 0.61  | 0.63       |\n\n---\n\n## 实践启示\n1. **RAG系统设计**  \n   - 需权衡文档数量与质量，优先检索短而精准的文档。\n   - 开发多文档处理机制（如冲突检测、信息聚合）。\n\n2. **研究方向**  \n   - 改进模型对多文档冗余/冲突信息的处理能力。\n   - 利用本文数据集（[GitHub](https://github.com/shaharl6000/MoreDocsSameLen)）进行模型训练与评估。\n\n---\n\n## 局限性\n- 未考虑**文档顺序**和**提示工程**的影响。\n- 实验文档数上限为20，需扩展至更大规模以贴近实际场景。\n- 仅测试基于MuSiQue的问答任务，需验证其他任务泛化性。\n\n> 引用格式：Levy et al. (2025). "More Documents, Same Length: Isolating the Challenge of Multiple Documents in RAG". arXiv:2503.04388v1\n```']


# 2503.04338v1

['\n\n```markdown\n# Summary: In-depth Analysis of Graph-based RAG in a Unified Framework\n\n## Authors\nYingli Zhou, Yaodong Su, Youran Sun, Shu Wang, Taotao Wang, Runyuan He, Yongwei Zhang, Sicong Liang, Xilin Liu, Yuchi Ma, Yixiang Fang  \n*Affiliations*: CUHK-Shenzhen, Huawei Cloud  \n\n## Abstract\n- **Problem**: Existing graph-based RAG methods lack systematic comparison under unified settings.  \n- **Solution**: Proposes a **unified framework** with 4 stages:  \n  1. Graph Building  \n  2. Index Construction  \n  3. Operator Configuration  \n  4. Retrieval & Generation  \n- **Key Contributions**:  \n  - Identifies new RAG variants (e.g., **VGraphRAG**, **CheapRAG**) outperforming SOTA.  \n  - Provides comprehensive analysis of 12 graph-based RAG methods across QA datasets.  \n  - Offers insights for future research.  \n\n---\n\n## Unified Framework\n1. **Graph Building**:  \n   - Converts corpus into structured graphs (Tree, KG, TKG, RKG).  \n   - *Example*: Knowledge Graphs (KG) extract entities/relationships; Rich KGs (RKG) add textual metadata.  \n\n2. **Index Construction**:  \n   - Builds vector databases for nodes, relationships, or communities.  \n   - *Key indices*: Node Index, Relationship Index, Community Index.  \n\n3. **Operator Configuration**:  \n   - Modular operator pool (19 operators) for retrieval:  \n     - Node/Relationship/Chunk/Subgraph/Community types.  \n   - Enables flexible method design by combining operators.  \n\n4. **Retrieval & Generation**:  \n   - Converts queries to retrieval primitives (entities/keywords/embeddings).  \n   - Generates answers via direct prompting or Map-Reduce strategies.  \n\n---\n\n## Experimental Analysis\n### **Datasets**  \n- **Specific QA** (factual): MultihopQA, HotpotQA, ALCE.  \n- **Abstract QA** (conceptual): Mix, Legal, Agriculture.  \n\n### **Key Findings**\n1. **Specific QA Performance**:  \n   - **RAPTOR** (tree-based) excels by leveraging hierarchical summaries.  \n   - Graph-based methods (e.g., **HippoRAG**) outperform vanilla RAG by 15–27% on complex queries.  \n   - *Limitation*: Rule-based chunk splitting degrades performance vs. expert-annotated chunks.  \n\n2. **Abstract QA Performance**:  \n   - **GGraphRAG** (community-based) achieves best results but is computationally expensive.  \n   - **CheapRAG** reduces token costs by 100× while maintaining quality.  \n\n3. **Token Costs**:  \n   - Graph construction for RKG consumes 40× more tokens than Tree.  \n   - Indexing large datasets (e.g., ALCE) incurs high token overheads.  \n\n4. **New SOTA Methods**:  \n   - **VGraphRAG**: Combines vector search with community/chunk retrieval (↑6.4% accuracy on MultihopQA).  \n   - **CheapRAG**: Optimizes GGraphRAG by prioritizing relevant communities (↓90% token cost).  \n\n---\n\n## Lessons & Opportunities\n### Lessons\n- **L1**: Use RAPTOR for specific QA; GGraphRAG for abstract QA (Figure 14).  \n- **L2**: Chunk quality (human-split > token-based) critically impacts performance.  \n- **L3**: High-level summaries (e.g., communities) improve multi-hop reasoning.  \n\n### Research Opportunities\n1. **Dynamic RAG**: Adapt to evolving knowledge sources (e.g., Wikipedia).  \n2. **Graph Quality Evaluation**: Develop metrics to pre-assess graph utility.  \n3. **Heterogeneous Data**: Extend RAG to PDFs, tables, and semi-structured data.  \n4. **Efficiency**: Optimize token costs for large-scale graphs.  \n\n---\n\n## Conclusion\n- Graph-based RAG enhances LLM accuracy by integrating structured knowledge.  \n- The proposed framework enables systematic comparison and novel method design.  \n- Open-source implementation and datasets available: [GitHub](https://github.com/JayLZhou/GraphRAG).  \n```']


# 2503.02879v1

['\n\n```markdown\n# Wikipedia in the Era of LLMs: Evolution and Risks  \n**Authors**: Siming Huang, Yuliang Xu, Mingmeng Geng, Yao Wan, Dongping Chen  \n**Institutions**: Huazhong University of Science and Technology, International School for Advanced Studies (SISSA)  \n\n## Key Findings  \n1. **Direct Impact on Wikipedia**  \n   - **Page Views**: A slight decline (1–2%) in views for scientific categories (e.g., Biology, Physics) observed post-2023, though causality with LLMs remains uncertain.  \n   - **Content Changes**:  \n     - Increased usage of LLM-preferred words like *"crucial"* (+0.04% frequency) and *"additionally"* (+0.07% frequency).  \n     - Linguistic shifts toward fewer auxiliary verbs (-15%), longer sentences (+20%), and reduced readability (e.g., Flesch-Kincaid Grade Level increased by 0.5).  \n   - **Estimated LLM Influence**: ~1–2% impact on pre-2020 articles in categories like Computer Science and Philosophy.  \n\n2. **Indirect Impact on NLP Tasks**  \n   - **Machine Translation**:  \n     - LLM-processed Wikipedia benchmarks inflate model scores (e.g., Facebook-NLLB BLEU scores rose from 72.39 to 93.38 for German).  \n     - Comparative rankings of models may reverse due to stylistic alignment with LLM outputs.  \n   - **Retrieval-Augmented Generation (RAG)**:  \n     - Accuracy dropped by 5–8% when using LLM-revised Wikipedia content due to information loss (e.g., fused sentences, omitted keywords).  \n     - Example: Original text specifying *"March 11, 2021"* revised to *"March 2021"* led to incorrect MCQ answers.  \n\n## Methodology  \n- **Data**: Analyzed 590,000+ Wikipedia articles (2019–2025) across 8 categories (e.g., Art, Sports) and 6,690 Featured Articles.  \n- **Simulations**:  \n  - **LLM Rewriting**: GPT-4o-mini and Gemini-1.5-Flash used to revise articles and generate synthetic benchmarks.  \n  - **Tasks Evaluated**:  \n    - Machine Translation: Flores-101 dataset translated into 11 languages.  \n    - RAG: 1,800+ Wikinews-based MCQs tested on GPT-3.5 and GPT-4o.  \n\n## Risks and Implications  \n- **Knowledge Pollution**: LLM-generated content risks degrading Wikipedia’s reliability for downstream tasks (e.g., RAG systems).  \n- **Evaluation Bias**: Machine translation benchmarks using LLM-influenced data may overestimate model performance.  \n- **Human-LLM Coevolution**: Wikipedia’s role as a bridge between human and AI collaboration necessitates safeguards against linguistic homogenization.  \n\n## Limitations  \n- **Formatting Noise**: Wikipedia’s structured templates (e.g., citations, infoboxes) may skew textual analysis.  \n- **Readability Metrics**: Reliance on traditional formulas (e.g., Flesch-Kincaid) rather than modern NLP-based assessments.  \n\n## Conclusion  \nWhile LLMs have not yet drastically altered Wikipedia’s core structure, early signs of influence highlight the need for:  \n1. **Monitoring frameworks** to track LLM-driven content changes.  \n2. **Revised benchmarks** for NLP tasks to account for LLM-polluted data.  \n3. **Community guidelines** to preserve Wikipedia’s integrity in the LLM era.  \n\n**Code & Data**: [GitHub Repository](https://github.com/HSM316/LLM_Wikipedia)  \n```']


# 2503.02922v1

['\n\n```markdown\n# 论文总结：基于图检索增强生成的开放域问答优化\n\n## 作者信息\n- **Joyce Cahoon** (Microsoft, Redmond)\n- **Prerna Singh** (Microsoft, Redmond)\n- **Nick Litombe** (Microsoft, Austin)\n- **Jonathan Larson** (Microsoft, Redmond)\n- **Ha Trinh** (Microsoft, Redmond)\n- **Yiwen Zhu** (Microsoft, Mountain View)\n- **Andreas Mueller** (Microsoft, Mountain View)\n- **Fotis Psallidas** (Microsoft, New York)\n- **Carlo Curino** (Microsoft, Redmond)\n\n---\n\n## 核心贡献\n1. **提出TREX方法**  \n   - 结合图结构与向量检索的混合RAG方案\n   - 成本效益比传统图方法（GraphRAG）降低10倍\n   - 支持OLTP（事实型）和OLAP（主题型）混合查询\n\n2. **基准测试创新**  \n   - 在4个异构数据集验证性能：\n     - HotPotQA（多跳问答）\n     - MSMARCO（搜索日志）\n     - 微软财报电话会议\n     - Kevin Scott技术领袖播客\n   - OLTP查询准确率最高达80.9%（HotPotQA）\n\n3. **新评估指标**  \n   - 针对OLAP查询的三大维度：\n     - **全面性**（信息覆盖深度）\n     - **多样性**（多视角覆盖）\n     - **赋能性**（决策支持力度）\n\n4. **实际案例验证**  \n   - 在微软技术客服系统实现65.2%准确率\n   - 比传统混合检索提升66%召回率\n\n---\n\n## 方法创新：TREX架构\n```mermaid\ngraph TD\n    A[文本分块] --> B[UMAP降维]\n    B --> C[GMM层次聚类]\n    C --> D[LLM递归摘要]\n    D --> E[混合检索库]\n    E --> F[RRF融合排序]\n```\n\n1. **层次聚类**  \n   - 使用UMAP将嵌入向量降维至5-10维\n   - 基于BIC准则自动选择GMM聚类数\n\n2. **混合检索机制**  \n   - 关键词搜索（BM25） + 语义搜索（余弦相似度）\n   - Reciprocal Rank Fusion (RRF) 融合结果\n\n3. **成本优化**  \n   - 索引成本比GraphRAG降低7-10倍\n   - 单查询平均成本<$0.01（GPT-4o）\n\n---\n\n## 关键实验结果\n| 数据集         | TREX准确率 | GraphRAG | 基准提升 |\n|----------------|------------|----------|----------|\n| HotPotQA       | 80.9%      | 54.9%    | +47%     |\n| 财报分析(OLAP) | 65.2%      | 58.7%    | +11%     |\n| 客服案例       | 65.2%      | 39.1%    | +66%     |\n\n---\n\n## 实践启示\n1. **检索质量决定上限**  \n   - 上下文选择比模型扩展更关键\n   - 冗余信息导致27%的答案质量下降\n\n2. **评估体系缺失**  \n   - 现有基准无法有效衡量OLAP响应质量\n   - 提出基于声明的细粒度验证框架\n\n3. **架构选型建议**  \n   - OLTP场景：优先向量检索+RAPTOR\n   - OLAP场景：图结构+社区摘要效果最佳\n\n---\n\n## 未来方向\n1. 动态路由机制（查询类型自动识别）\n2. 知识图谱与向量空间的联合嵌入\n3. 基于强化学习的检索-生成协同优化\n```']


# 2503.02800v2

['\n\n```markdown\n# RAAD-LLM: Adaptive Anomaly Detection Using LLMs and RAG Integration\n\n## Summary\n\n### Problem Statement\n- **Challenges**: Anomaly detection in industrial settings faces data sparsity, concept drift, and the need for domain expertise integration.\n- **Objective**: Develop an adaptive, transferable framework for predictive maintenance (PdM) without dataset-specific fine-tuning.\n\n### Methodology\n- **Framework**: RAAD-LLM combines:\n  - **Pretrained LLMs** (e.g., Llama 3.1 8B) for zero-shot reasoning.\n  - **Retrieval-Augmented Generation (RAG)** to dynamically retrieve domain-specific knowledge.\n  - **Adaptability Mechanism**: Updates baseline "normal" behavior using sliding windows and statistical process control (SPC).\n- **Key Components**:\n  - **SPC Techniques**: MAMR and MEWMA filter anomalies and refine control limits.\n  - **DFT Processing**: Isolates dominant frequency components to enhance signal clarity.\n  - **Multimodal Inputs**: Enriches time-series data with semantic context via RAG.\n  - **Binarization Function**: Maps LLM outputs to anomaly labels (0/1) based on domain rules.\n\n### Evaluation\n- **Datasets**:\n  1. **Real-World Use-Case**: Screen pack failures in a plastics manufacturing plant (3 process variables).\n  2. **Skoltech Anomaly Benchmark (SKAB)**: Public dataset with labeled sensor data.\n- **Results**:\n  - **Manufacturing Data**: Accuracy improved from 70.7% (AAD-LLM) to **89.1%** (RAAD-LLM).\n  - **SKAB Dataset**: Accuracy of 72% (F1=0.74), outperforming LSTMCaps and MSCRED.\n  - **MAR Minimization**: Critical for safety, RAAD-LLM achieved **11.43% MAR** vs. 18.74% for LSTMCaps.\n- **RAAD-LLMv2**: Integrated LlamaIndex for automated context retrieval but showed slightly lower performance (73% accuracy on manufacturing data).\n\n### Key Contributions\n1. **Zero-Shot Transferability**: Leverages pretrained LLMs without fine-tuning.\n2. **Dynamic Knowledge Integration**: RAG retrieves domain-specific thresholds and correlations.\n3. **Adaptability**: Continuously updates normal operational baselines to handle concept drift.\n4. **Multimodality**: Combines time-series analysis with semantic context for collaborative decision-making.\n\n### Conclusion & Future Work\n- **Impact**: RAAD-LLM bridges the gap between data-driven models and human expertise, enhancing PdM in industrial applications.\n- **Limitations**: Manual context restructuring in RAAD-LLM; scalability challenges in RAAD-LLMv2.\n- **Future Directions**:\n  - Automate domain context integration.\n  - Extend to real-time data streams and other domains (e.g., healthcare, finance).\n  - Optimize hybrid approaches for context retrieval.\n\n---\n\n## Key Figures\n| Metric                | RAAD-LLM (Manufacturing) | RAAD-LLM (SKAB) | RAAD-LLMv2 (Manufacturing) |\n|-----------------------|--------------------------|-----------------|----------------------------|\n| **Accuracy**          | 89.1%                   | 72%             | 73%                        |\n| **Precision**          | 97%                     | 63%             | 96%                        |\n| **Recall**             | 88%                     | 89%             | 66%                        |\n| **F1 Score**           | 92%                     | 74%             | 78%                        |\n\n*Comparison with baselines: RAAD-LLM outperformed specialized models (e.g., LSTMCaps, MSCRED) in MAR and F1 scores.*\n```']


