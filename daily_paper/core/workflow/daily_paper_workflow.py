from daily_paper.core.pipeline import DAGPipeline
from daily_paper.core.operators.datasource.arxiv import ArxivSource
from daily_paper.core.operators.processor.paper_reader import PaperReader
from daily_paper.core.operators.processor.llm_summarizer import LLMSummarizer
from daily_paper.core.operators.processor.custom_processor import CustomProcessor
from daily_paper.core.operators.state.pending import (
    FilterFinishedIDs,
    MarkIDsAsFinished,
    InsertPendingIDs,
)
from daily_paper.core.models import Paper, PaperWithSummary
from daily_paper.core.config import LLMConfig
from daily_paper.core.operators.sink.feishu import FeishuPusher
from daily_paper.core.config import Config
from daily_paper.core.common import logger
from daily_paper.core.operators.storage.local_storage import (
    LocalStorageWriter,
    LocalStorageReader,
)
import os
import asyncio
import argparse
from typing import Tuple, List, Any
from dataclasses import asdict
from daily_paper.core.operators.processor.abstract_based_llm_filter import AbstractBasedLLMFilter
import logging

def id_getter(x: Paper):
    return x.id

async def create_paper_filter_pipeline(config: Config) -> DAGPipeline:
    """åˆ›å»ºè®ºæ–‡è¿‡æ»¤pipeline"""
    pipeline = DAGPipeline()

    pipeline.add_operator(
        name="arxiv_source",
        operator=ArxivSource(
            topic=config.arxiv_topic_list, search_offset=config.arxiv_search_offset, search_limit=config.arxiv_search_limit
        ),
        dependencies=None,
    )

    pipeline.add_operator(
        name="filter_arxiv_papers",
        operator=FilterFinishedIDs(
            base_dir=os.path.join(config.storage.base_path, "state"),
            namespace="arxiv_llm_filter",
            id_getter=id_getter,
        ),
        dependencies=["arxiv_source"],
    )

    pipeline.add_operator(
        name="llm_filter",
        operator=AbstractBasedLLMFilter(config.llm, config.llm_filter_topic),
        dependencies=["filter_arxiv_papers"],
    )

    def kv_getter(x: Tuple[Paper, bool]):
        filtered = x[1]
        if not filtered:
            return x[0].id, asdict(x[0])
        else:
            return x[0].id, None

    pipeline.add_operator(
        name="save_filtered_papers",
        operator=LocalStorageWriter(
            storage_dir=os.path.join(config.storage.base_path, "filtered_papers"),
            storage_namespace="filtered_papers",
            key_value_getter=kv_getter,
        ),
        dependencies=["llm_filter"],
    )

    def paper_with_filter_status_id_getter(x: Tuple[Paper, bool]):
        return x[0].id

    pipeline.add_operator(
        name="mark_filtered_papers",
        operator=MarkIDsAsFinished(
            base_dir=os.path.join(config.storage.base_path, "state"),
            namespace="arxiv_llm_filter",
            id_getter=paper_with_filter_status_id_getter,  
        ),
        dependencies=["save_filtered_papers"],
    )
    
    return pipeline

async def create_paper_summarize_pipeline(config: Config) -> DAGPipeline:
    """åˆ›å»ºè®ºæ–‡å¤„ç†pipeline

    åŒ…å«ä»¥ä¸‹æ­¥éª¤ï¼š
    1. ä»Arxivè·å–è®ºæ–‡
    2. ä½¿ç”¨PaperReaderè¯»å–è®ºæ–‡
    3. ä½¿ç”¨LLMSummarizeræ€»ç»“è®ºæ–‡

    Returns:
        DAGPipeline: é…ç½®å¥½çš„pipelineå®ä¾‹
    """
    pipeline = DAGPipeline()

    if not config.enable_llm_filter:
        # æ·»åŠ æ•°æ®æºç®—å­
        pipeline.add_operator(
            name="paper_source",
            operator=ArxivSource(
                topic=config.arxiv_topic_list, search_offset=config.arxiv_search_offset, search_limit=config.arxiv_search_limit
            ),
            dependencies=None,
        )
    else:
        def convert_to_paper(key: str, value: dict):
            return Paper(**value)

        pipeline.add_operator(
            name="paper_source",
            operator=LocalStorageReader(
                storage_dir=os.path.join(config.storage.base_path, "filtered_papers"),
                storage_namespace="filtered_papers",
                value_reader=convert_to_paper,
            ),
            dependencies=None,
        )

    pipeline.add_operator(
        name="filter_pending_ids",
        operator=FilterFinishedIDs(
            base_dir=os.path.join(config.storage.base_path, "state"),
            namespace="arxiv",
            id_getter=id_getter,
        ),
        dependencies=["paper_source"],
    )

    pipeline.add_operator(
        name="limit_batch_size",
        operator=CustomProcessor(lambda x: x[:config.process_batch_size]),
        dependencies=["filter_pending_ids"],
    )

    # only read the unprocessed papers
    pipeline.add_operator(
        name="paper_reader",
        operator=PaperReader(os.path.join(config.storage.base_path, "paper_caches")),
        dependencies=["limit_batch_size"],
    )

    # æ·»åŠ è®ºæ–‡æ€»ç»“ç®—å­
    pipeline.add_operator(
        name="paper_summarizer",
        operator=LLMSummarizer(config.llm),
        dependencies=["paper_reader"],
    )

    def kv_getter(x: PaperWithSummary):
        return x.id, asdict(x)

    pipeline.add_operator(
        name="save_paper_summaries",
        operator=LocalStorageWriter(
            storage_dir=os.path.join(config.storage.base_path, "paper_summaries"),
            storage_namespace="paper_summaries",
            key_value_getter=kv_getter,
        ),
        dependencies=["paper_summarizer"],
    )

    # mark the processed papers as finished
    pipeline.add_operator(
        name="mark_processed_papers",
        operator=MarkIDsAsFinished(
            base_dir=os.path.join(config.storage.base_path, "state"),
            namespace="arxiv",
            id_getter=id_getter,
        ),
        dependencies=["save_paper_summaries"],
    )

    return pipeline


async def create_paper_push_pipeline(config: Config) -> DAGPipeline:
    """åˆ›å»ºè®ºæ–‡æ¨é€pipeline

    åŒ…å«ä»¥ä¸‹æ­¥éª¤ï¼š
    1. ä»æœ¬åœ°å­˜å‚¨è¯»å–è®ºæ–‡æ‘˜è¦
    2. ä½¿ç”¨FeishuPushç®—å­æ¨é€è®ºæ–‡æ‘˜è¦åˆ°é£ä¹¦
    3. å°†æ¨é€æˆåŠŸçš„è®ºæ–‡æ‘˜è¦æ ‡è®°ä¸ºå·²æ¨é€
    """
    pipeline = DAGPipeline()

    def convert_to_paper_with_summary(key: str, value: dict):
        return PaperWithSummary(**value)

    pipeline.add_operator(
        name="read_paper_summaries",
        operator=LocalStorageReader(
            storage_dir=os.path.join(config.storage.base_path, "paper_summaries"),
            storage_namespace="paper_summaries",
            value_reader=convert_to_paper_with_summary,
        ),
        dependencies=None,
    )

    # filter the papers that have been pushed
    pipeline.add_operator(
        name="filter_pushed_papers",
        operator=FilterFinishedIDs(
            base_dir=os.path.join(config.storage.base_path, "state"),
            namespace="push",
            id_getter=id_getter,
        ),
        dependencies=["read_paper_summaries"],
    )

    def order_by_update_date(x: List[PaperWithSummary]) -> List[PaperWithSummary]:
        return sorted(x, key=lambda y: y.update_date)

    pipeline.add_operator(
        name="order_by_update_date",
        operator=CustomProcessor(order_by_update_date),
        dependencies=["filter_pushed_papers"],
    )

    def title_and_content_getter(x: PaperWithSummary) -> Tuple[str, str]:
        title = "ğŸ“„ æ–°è®ºæ–‡æ¨è"
        content = f"**{x.title}**\n"
        content += f"**æ›´æ–°æ—¶é—´**: {x.update_date}\n\n"
        content += f"ğŸ‘¤ {x.authors}\n\n"
        content += f"ğŸ’¡ AIæ€»ç»“ï¼š{x.summary}...\n\n"
        content += f"---\n"
        content += f"ğŸ“ [è®ºæ–‡åŸæ–‡]({x.url})"
        return title, content

    pipeline.add_operator(
        name="push_paper_summaries",
        operator=FeishuPusher(config.feishu_webhook_url, title_and_content_getter),
        dependencies=["order_by_update_date"],
    )

    def filter_out_failed_papers(
        x: List[Tuple[PaperWithSummary, bool]],
    ) -> List[PaperWithSummary]:
        return [y[0] for y in x if y[1] is True]

    pipeline.add_operator(
        name="filter_out_push_failed_papers",
        operator=CustomProcessor(filter_out_failed_papers),
        dependencies=["push_paper_summaries"],
    )

    pipeline.add_operator(
        name="mark_pushed_papers",
        operator=MarkIDsAsFinished(
            base_dir=os.path.join(config.storage.base_path, "state"),
            namespace="push",
            id_getter=id_getter,
        ),
        dependencies=["filter_out_push_failed_papers"],
    )

    return pipeline


async def run_paper_summarize_pipeline(config_path: str):
    """è¿è¡Œè®ºæ–‡å¤„ç†pipeline"""
    config = Config.from_yaml(config_path)
    total_results = []
    # TODO(ysj): use sink to collect results
    while True:
      pipeline: DAGPipeline = await create_paper_summarize_pipeline(config)
      results = await pipeline.execute()
      logger.info(f"Paper Summarize Pipeline small batch completed with {len(results)} results")
      total_results.extend(results)
      if len(results) == 0:
        break

    logger.info(f"Paper Summarize Pipeline completed with {len(total_results)} results")

    return total_results


async def run_paper_push_pipeline(config_path: str):
    config = Config.from_yaml(config_path)
    pipeline: DAGPipeline = await create_paper_push_pipeline(config)
    results = await pipeline.execute()
    logger.info(f"Paper Push Pipeline completed with {len(results)} results")
    return results

async def run_paper_filter_pipeline(config_path: str):
    config = Config.from_yaml(config_path)
    pipeline: DAGPipeline = await create_paper_filter_pipeline(config)
    results = await pipeline.execute()
    logger.info(f"Paper Filter Pipeline completed with {len(results)} results")
    return results

if __name__ == "__main__":
    args = argparse.ArgumentParser()
    args.add_argument("--config", type=str, default="config.yaml")
    args = args.parse_args()

    # é…ç½®æ—¥å¿—è®°å½•
    logging.basicConfig(
        level=logging.INFO,  # è®¾ç½®æ—¥å¿—çº§åˆ«ä¸º INFO
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=[
            logging.StreamHandler(),  # è¾“å‡ºåˆ°æ§åˆ¶å°
        ]
    )

    arxiv_logger = logging.getLogger('arxiv')
    arxiv_logger.setLevel(logging.INFO)


    # asyncio.run(run_paper_filter_pipeline(args.config))
    # asyncio.run(run_paper_summarize_pipeline(args.config))
    asyncio.run(run_paper_push_pipeline(args.config))