import logging
from typing import TypedDict, Optional
import datetime
import arxiv
import pandas as pd
from pathlib import Path
from datetime import datetime
import os
from PyPDF2 import PdfReader
import dspy
import pandas as pd
from pathlib import Path
import requests
from tqdm import tqdm  # æ–°å¢è¿›åº¦æ¡å¯¼å…¥
from functools import wraps
import time
import asyncio
from concurrent.futures import ThreadPoolExecutor
import ast

ARXIV_URL = "http://arxiv.org/"

LLM_API_KEY = os.getenv("LLM_API_KEY")
LLM_BASE_URL = os.getenv("LLM_BASE_URL")
CHAT_MODEL_NAME = os.getenv("CHAT_MODEL_NAME")
FEISHU_WEBHOOK_URL = os.getenv("FEISHU_WEBHOOK_URL")

FILTER_FILE_NAME = "data/daily_papers.parquet"

def sync_timer(func):
    @wraps(func)
    def wrapper(*args, **kwargs):
        start_time = time.time()
        result = func(*args, **kwargs)
        end_time = time.time()
        elapsed_time = (end_time - start_time) * 1000
        print(f"function: {func.__name__} execution time: {elapsed_time:.2f} millisecond")
        return result
    return wrapper

class ArxivPaper(TypedDict):
    paper_id: str
    paper_title: str
    paper_url: str
    paper_abstract: str
    paper_authors: str
    paper_first_author: str
    primary_category: str
    publish_time: datetime.date
    update_time: datetime.date
    comments: Optional[str]

def get_authors(authors, first_author = False):
    if first_author:
        return str(authors[0])  # æ˜¾å¼è½¬æ¢ä¸ºå­—ç¬¦ä¸²
    return ", ".join(str(author) for author in authors)  # ç¡®ä¿æ‰€æœ‰å…ƒç´ éƒ½æ˜¯å­—ç¬¦ä¸²

def get_daily_papers(query, max_results) -> dict[str, ArxivPaper]:
    paper_result = {}
    search_engine = arxiv.Search(
        query = query,
        max_results = max_results,
        sort_by = arxiv.SortCriterion.SubmittedDate
    )

    for result in search_engine.results():
        paper_id            = result.get_short_id()
        paper_title         = result.title
        paper_url           = result.entry_id
        paper_abstract      = result.summary.replace("\n"," ")
        paper_authors       = get_authors(result.authors)
        paper_first_author  = get_authors(result.authors, first_author = True)
        primary_category    = result.primary_category
        publish_time        = result.published.date()
        update_time         = result.updated.date()
        comments            = result.comment


        logging.info(f"Time = {update_time} title = {paper_title} author = {paper_first_author}")

        # eg: 2108.09112v1 -> 2108.09112
        ver_pos = paper_id.find('v')
        if ver_pos == -1:
            paper_key = paper_id
        else:
            paper_key = paper_id[0:ver_pos]
        paper_url = ARXIV_URL + 'abs/' + paper_key

        arxiv_paper = ArxivPaper(
          paper_id=paper_id,
          paper_title=paper_title,
          paper_url=paper_url,
          paper_abstract=paper_abstract,
          paper_authors=paper_authors,
          paper_first_author=paper_first_author,
          primary_category=primary_category,
          publish_time=publish_time,
          update_time=update_time,
          comments=comments
        )
        paper_result[paper_key] = arxiv_paper

    return paper_result

def save_to_parquet(papers: dict[str, ArxivPaper]):
    """ä¿å­˜è®ºæ–‡æ•°æ®åˆ°parquetæ–‡ä»¶ï¼ˆå¢åŠ pushedå­—æ®µï¼‰"""
    Path("data").mkdir(exist_ok=True)
    filename = FILTER_FILE_NAME
    
    # è¯»å–å·²æœ‰æ•°æ®ï¼ˆå¦‚æœæ–‡ä»¶å­˜åœ¨ï¼‰
    existing_df = pd.DataFrame()
    if Path(filename).exists():
        try:
            existing_df = pd.read_parquet(filename)
        except Exception as e:
            logging.warning(f"Error reading existing file: {str(e)}")
    
    # åˆå¹¶æ–°æ—§æ•°æ®æ—¶æ·»åŠ pushedå­—æ®µ
    new_df = pd.DataFrame.from_dict(papers, orient='index')
    new_df['summary'] = None
    new_df['pushed'] = False  # æ–°å¢æ¨é€çŠ¶æ€å­—æ®µ
    combined_df = pd.concat([existing_df, new_df], ignore_index=False)
    
    # å»é‡ï¼ˆä¿ç•™æœ€åå‡ºç°çš„è®°å½•ï¼‰å¹¶ä¿å­˜
    combined_df = combined_df[~combined_df.index.duplicated(keep='last')]
    combined_df.to_parquet(filename, engine='pyarrow')

def send_to_feishu(paper: ArxivPaper, summary: str) -> bool:
    """å‘é€å•ç¯‡è®ºæ–‡åˆ°é£ä¹¦ï¼ˆè¿”å›æ˜¯å¦æˆåŠŸï¼‰"""
    if not FEISHU_WEBHOOK_URL:
        logging.error("é£ä¹¦Webhookåœ°å€æœªé…ç½®")
        return

    formatted_summary = summary.replace("\\n", "\n")
    
    message = {
        "msg_type": "interactive",
        "card": {
            "elements": [{
                "tag": "div",
                "text": {
                    "content": f"**{paper['paper_title']}**\n"
                               f"**æ›´æ–°æ—¶é—´**: {paper['update_time']}\n\n"
                               f"ğŸ‘¤ {paper['paper_authors']}\n\n"
                               f"ğŸ’¡ AIæ€»ç»“ï¼š{formatted_summary}...\n\n"
                               f"---\n"
                               f"ğŸ“ [è®ºæ–‡åŸæ–‡]({paper['paper_url']})",
                    "tag": "lark_md"
                }
            }],
            "header": {
                "title": {
                    "content": "ğŸ“„ æ–°è®ºæ–‡æ¨è",
                    "tag": "plain_text"
                }
            }
        }
    }

    try:
        send_to_feishu_with_retry(message)
        logging.info(f"é£ä¹¦æ¨é€æˆåŠŸ: {paper['paper_id']}")
        return True
    except Exception as e:
        logging.error(f"é£ä¹¦æ¨é€å¤±è´¥: {str(e)}")
        return False

def push_to_feishu(df: pd.DataFrame) -> pd.DataFrame:
    """æ‰¹é‡æ¨é€æœªå‘é€è®ºæ–‡å¹¶æ›´æ–°çŠ¶æ€"""
    # ç­›é€‰éœ€è¦æ¨é€çš„è®ºæ–‡
    to_push = df[(df['pushed'] == False) & 
                (df['summary'].notna())].copy()
    
    if to_push.empty:
        logging.info("æ²¡æœ‰éœ€è¦æ¨é€çš„æ–°è®ºæ–‡")
        return df
    
    # æŒ‰æ—¶é—´æ’åºï¼ˆæ—§åˆ°æ–°ï¼‰
    sorted_df = to_push.sort_values('update_time', ascending=True)
    
    # æ‰¹é‡å¤„ç†æ¨é€
    success_indices = []
    for index, row in sorted_df.iterrows():
        paper = ArxivPaper(
            paper_id=row['paper_id'],
            paper_title=row['paper_title'],
            paper_url=row['paper_url'],
            paper_abstract=row['paper_abstract'],
            paper_authors=row['paper_authors'],
            paper_first_author=row['paper_first_author'],
            primary_category=row['primary_category'],
            publish_time=row['publish_time'],
            update_time=row['update_time'],
            comments=row['comments']
        )
        if send_to_feishu(paper, row['summary']):
            success_indices.append(index)
    
    # æ‰¹é‡æ›´æ–°æ¨é€çŠ¶æ€
    if success_indices:
        df.loc[success_indices, 'pushed'] = True
        df.to_parquet(FILTER_FILE_NAME, engine='pyarrow')
        logging.info(f"æˆåŠŸæ›´æ–°{len(success_indices)}ç¯‡è®ºæ–‡æ¨é€çŠ¶æ€")
    
    return df

# ä¸»æµç¨‹ä¿®æ”¹
if __name__ == "__main__":
    # é…ç½®dspy
    lm = dspy.LM("openai/" + CHAT_MODEL_NAME, api_base=LLM_BASE_URL, api_key=LLM_API_KEY, temperature=0.2)
    dspy.configure(lm=lm)

    # è·å–ä»Šæ—¥è®ºæ–‡
    new_papers = get_daily_papers("\"RAG\" OR \"Retrieval-Augmented Generation\"", 200)

    # è¿‡æ»¤å·²å­˜åœ¨è®ºæ–‡
    filtered_papers = filter_existing_papers(new_papers)

    save_to_parquet(filtered_papers)
    print(f"ä¿å­˜äº†{len(filtered_papers)}ç¯‡æ–°è®ºæ–‡")
    
    # è¯»å–ä¿å­˜çš„è®ºæ–‡æ•°æ®
    df = pd.read_parquet(FILTER_FILE_NAME)

    # æ·»åŠ ç¼ºå¤±çš„summaryåˆ—ï¼ˆå…¼å®¹æ—§æ•°æ®ï¼‰
    if 'summary' not in df.columns:
        df['summary'] = None

    # è¿‡æ»¤æ‰å·²ç»æœ‰summaryå­—æ®µçš„è®ºæ–‡
    papers_without_summary = df[df['summary'].isna()]

    print(f"éœ€è¦å¤„ç†{len(papers_without_summary)}ç¯‡æ–°è®ºæ–‡")

    # åˆ›å»ºçº¿ç¨‹æ± æ‰§è¡Œå™¨
    executor = ThreadPoolExecutor(max_workers=20)
    # ä¿®å¤äº‹ä»¶å¾ªç¯åˆ›å»ºæ–¹å¼
    loop = asyncio.new_event_loop()
    asyncio.set_event_loop(loop)
    
    # å‡†å¤‡æ‰€æœ‰ä»»åŠ¡
    tasks = []
    for index, row in papers_without_summary.iterrows():
        paper = ArxivPaper(
            paper_id=row['paper_id'],
            paper_title=row['paper_title'],
            paper_url=row['paper_url'],
            paper_abstract=row['paper_abstract'],
            paper_authors=row['paper_authors'],
            paper_first_author=row['paper_first_author'],
            primary_category=row['primary_category'],
            publish_time=row['publish_time'],
            update_time=row['update_time'],
            comments=row['comments']
        )
        tasks.append(process_single_paper(executor, lm, paper, index))
    
    # ä½¿ç”¨tqdmæ˜¾ç¤ºå¹¶å‘ä»»åŠ¡è¿›åº¦
    from tqdm.asyncio import tqdm_asyncio
    results = loop.run_until_complete(
        tqdm_asyncio.gather(*tasks, desc="å¹¶å‘å¤„ç†è®ºæ–‡", total=len(tasks))
    )
    
    # æ‰¹é‡æ›´æ–°ç»“æœ
    for index, summary in results:
        df.at[index, 'summary'] = summary

    # ä¿å­˜æ›´æ–°åçš„DataFrame
    df.to_parquet(FILTER_FILE_NAME, engine='pyarrow')
    
    # æ–°å¢é£ä¹¦æ¨é€ï¼ˆåªæ¨é€æœ¬æ¬¡å¤„ç†çš„è®ºæ–‡ï¼‰
    # æŒ‰update_timeä»æ—§åˆ°æ–°æ’åº
    sorted_papers = df.loc[papers_without_summary.index].sort_values('update_time', ascending=True)
    
    for index, row in sorted_papers.iterrows():
        if pd.notna(row['summary']):
            paper = ArxivPaper(
                paper_id=row['paper_id'],
                paper_title=row['paper_title'],
                paper_url=row['paper_url'],
                paper_abstract=row['paper_abstract'],
                paper_authors=row['paper_authors'],
                paper_first_author=row['paper_first_author'],
                primary_category=row['primary_category'],
                publish_time=row['publish_time'],
                update_time=row['update_time'],
                comments=row['comments']
            )
            send_to_feishu(paper, row['summary'], index)  # ä¼ å…¥dfç´¢å¼•

    # ç¤ºä¾‹ï¼šåˆ†æç¬¬ä¸€ç¯‡è¿‡æ»¤åçš„è®ºæ–‡æ˜¯å¦å±äºç‰¹å®šé¢†åŸŸ
    # if filtered_papers:
    #     first_paper = next(iter(filtered_papers.values()))
    #     is_in_domain = analyze_paper(first_paper, "RAG")
    #     print(f"ç¬¬ä¸€ç¯‡è®ºæ–‡æ˜¯å¦å±äºRAGé¢†åŸŸ: {is_in_domain}")

def filter_existing_papers(new_papers: dict[str, ArxivPaper]) -> dict[str, ArxivPaper]:
    """è¿‡æ»¤å·²å­˜åœ¨çš„è®ºæ–‡ï¼ˆå•æ–‡ä»¶ç‰ˆæœ¬ï¼‰"""
    existing_ids = set()
    filename = FILTER_FILE_NAME
    
    # æ£€æŸ¥å¹¶è¯»å–å•ä¸ªæ–‡ä»¶
    if Path(filename).exists():
        try:
            df = pd.read_parquet(filename)
            if not df.empty and 'paper_id' in df.columns:
                existing_ids.update(df['paper_id'].tolist())
        except Exception as e:
            logging.warning(f"Error reading {filename}: {str(e)}")
    
    # è¿‡æ»¤æ–°è®ºæ–‡
    return {k: v for k, v in new_papers.items() if v['paper_id'] not in existing_ids}

class PaperAnalysis(dspy.Signature):
    """åˆ†æè®ºæ–‡æ‘˜è¦å¹¶åˆ¤æ–­æ˜¯å¦å±äºæŸä¸€ä¸ªç‰¹å®šé¢†åŸŸ"""
    input_paper_text: str = dspy.InputField(desc="è®ºæ–‡çš„æ‘˜è¦")
    input_domain: str = dspy.InputField(desc="é¢†åŸŸå")
    output_domain: bool = dspy.OutputField(desc="æ˜¯å¦å±äºç”¨æˆ·ç»™å®šçš„é¢†åŸŸ, è¿”å›trueæˆ–false")

def analyze_paper(paper: ArxivPaper, domain: str) -> bool:
    """
    ä½¿ç”¨PaperAnalysisåˆ†æè®ºæ–‡æ˜¯å¦å±äºç‰¹å®šé¢†åŸŸã€‚
    
    :param paper: è¦åˆ†æçš„è®ºæ–‡ï¼Œç±»å‹ä¸ºArxivPaper
    :param domain: è¦åˆ¤æ–­çš„é¢†åŸŸå
    :return: å¦‚æœè®ºæ–‡å±äºè¯¥é¢†åŸŸè¿”å›Trueï¼Œå¦åˆ™è¿”å›False
    """
    analysis = PaperAnalysis()
    result = analysis(input_paper_text=paper['paper_abstract'], input_domain=domain)
    return result.output_domain

def summarize_paper(lm, paper_text) -> str:
    # ä¿®æ­£å†’å·ä¸ºè‹±æ–‡æ ¼å¼ï¼Œä½¿ç”¨æ ‡å‡†ç­¾åè¯­æ³•
    prompt = f"ç”¨ä¸­æ–‡å¸®æˆ‘ä»‹ç»ä¸€ä¸‹è¿™ç¯‡æ–‡ç« : {paper_text}"
    summary = lm(prompt)
    return summary[0]

def extract_text_from_pdf(pdf_path):
    """æå–PDFæ–‡æœ¬å†…å®¹ï¼ˆå¢åŠ åŒè§£æå¼•æ“ï¼‰"""
    try:
        # å°è¯•ä½¿ç”¨PyPDF2è§£æ
        with open(pdf_path, 'rb') as f:
            reader = PdfReader(f)
            return '\n'.join([page.extract_text() for page in reader.pages])
    except Exception as pdf_error:
        print(f"PyPDF2è§£æå¤±è´¥ï¼Œå°è¯•å¤‡ç”¨è§£æå¼•æ“: {pdf_path}")
        try:
            # å¤‡é€‰æ–¹æ¡ˆ1ï¼šä½¿ç”¨pdfplumberï¼ˆéœ€è¦å®‰è£…ï¼‰
            import pdfplumber
            with pdfplumber.open(pdf_path) as pdf:
                return '\n'.join([page.extract_text() for page in pdf.pages])
        except Exception as plumber_error:
            try:
                # å¤‡é€‰æ–¹æ¡ˆ2ï¼šä½¿ç”¨PyMuPDFï¼ˆéœ€è¦å®‰è£…ï¼‰
                import fitz  # PyMuPDFçš„å¯¼å…¥åç§°
                doc = fitz.open(pdf_path)
                return '\n'.join([page.get_text() for page in doc])
            except Exception as fitz_error:
                error_msg = (
                    f"PDFè§£æå…¨éƒ¨å¤±è´¥: {pdf_path}\n"
                    f"PyPDF2é”™è¯¯: {str(pdf_error)}\n"
                    f"pdfplumberé”™è¯¯: {str(plumber_error)}\n"
                    f"PyMuPDFé”™è¯¯: {str(fitz_error)}"
                )
                print(error_msg)
                return ""

def download_paper(url: str, paper_id: str, save_dir: str, retries=3):
    """ä¸‹è½½å¹¶ä¿å­˜PDFè®ºæ–‡ï¼ˆå¢åŠ é‡è¯•æœºåˆ¶ï¼‰"""
    os.makedirs(save_dir, exist_ok=True)
    file_path = os.path.join(save_dir, f"{paper_id}.pdf")
    
    if os.path.exists(file_path):
        print(f"æ–‡ä»¶å·²å­˜åœ¨ï¼Œè·³è¿‡ä¸‹è½½: {paper_id}")
        return
    
    for attempt in range(retries):
        try:
            response = requests.get(url, stream=True, timeout=30)
            response.raise_for_status()
            
            # å¢åŠ æ–‡ä»¶å®Œæ•´æ€§æ ¡éªŒ
            total_size = int(response.headers.get('content-length', 0))
            downloaded = 0
            
            with open(file_path, 'wb') as f:
                for chunk in response.iter_content(chunk_size=8192):
                    downloaded += len(chunk)
                    f.write(chunk)
                    
            # ç®€å•æ ¡éªŒæ–‡ä»¶å®Œæ•´æ€§
            if total_size > 0 and downloaded != total_size:
                raise IOError("æ–‡ä»¶å¤§å°ä¸åŒ¹é…ï¼Œå¯èƒ½ä¸‹è½½ä¸å®Œæ•´")
                
            print(f"æˆåŠŸä¸‹è½½: {paper_id}")
            return
        except Exception as e:
            if attempt < retries - 1:
                print(f"ä¸‹è½½å¤±è´¥ {paper_id}ï¼Œç¬¬{attempt+1}æ¬¡é‡è¯•...")
                time.sleep(2)
            else:
                print(f"ä¸‹è½½æœ€ç»ˆå¤±è´¥ {paper_id}: {str(e)}")
                try:
                    os.remove(file_path)
                except:
                    pass

async def process_single_paper(executor, lm, paper, row_index):
    """å¹¶å‘å¤„ç†å•ç¯‡è®ºæ–‡çš„å¼‚æ­¥ä»»åŠ¡"""
    loop = asyncio.get_event_loop()
    
    # ä¸‹è½½è®ºæ–‡ï¼ˆä½¿ç”¨çº¿ç¨‹æ± æ‰§è¡Œé˜»å¡IOï¼‰
    pdf_url = paper['paper_url'].replace('abs', 'pdf')
    await loop.run_in_executor(executor, download_paper, pdf_url, paper['paper_id'], 'papers')
    
    # æå–æ–‡æœ¬
    pdf_path = os.path.join('papers', f"{paper['paper_id']}.pdf")
    paper_text = await loop.run_in_executor(executor, extract_text_from_pdf, pdf_path)
    
    # æ€»ç»“è®ºæ–‡
    summary = await loop.run_in_executor(executor, summarize_paper, lm, paper_text)
    
    return row_index, summary

from tenacity import retry, wait_exponential, stop_after_attempt

@retry(stop=stop_after_attempt(100), wait=wait_exponential(multiplier=1, min=1, max=10))
def send_to_feishu_with_retry(message):
    """å¸¦é‡è¯•æœºåˆ¶çš„é£ä¹¦æ¶ˆæ¯æ¨é€"""
    response = requests.post(
        FEISHU_WEBHOOK_URL,
        json=message,
        timeout=10
    )
    response.raise_for_status()