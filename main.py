import logging
from typing import TypedDict, Optional
import arxiv
import pandas as pd
from pathlib import Path
import datetime
import os
from PyPDF2 import PdfReader
import dspy
import pandas as pd
from pathlib import Path
import requests
from tqdm import tqdm  # æ–°å¢è¿›åº¦æ¡å¯¼å…¥
from functools import wraps
import time
import asyncio
from concurrent.futures import ThreadPoolExecutor
import ast
import argparse

ARXIV_URL = "http://arxiv.org/"

LLM_API_KEY = os.getenv("LLM_API_KEY")
LLM_BASE_URL = os.getenv("LLM_BASE_URL")
CHAT_MODEL_NAME = os.getenv("CHAT_MODEL_NAME")
FEISHU_WEBHOOK_URL = os.getenv("FEISHU_WEBHOOK_URL")
MAX_PAPER_TEXT_LENGTH = 128000

def sync_timer(func):
    @wraps(func)
    def wrapper(*args, **kwargs):
        start_time = time.time()
        result = func(*args, **kwargs)
        end_time = time.time()
        elapsed_time = (end_time - start_time) * 1000
        print(f"function: {func.__name__} execution time: {elapsed_time:.2f} millisecond")
        return result
    return wrapper

class ArxivPaper(TypedDict):
    paper_id: str
    paper_title: str
    paper_url: str
    paper_abstract: str
    paper_authors: str
    paper_first_author: str
    primary_category: str
    publish_time: datetime.date
    update_time: datetime.date
    comments: Optional[str]

def get_authors(authors, first_author = False):
    if first_author:
        return str(authors[0])  # æ˜¾å¼è½¬æ¢ä¸ºå­—ç¬¦ä¸²
    return ", ".join(str(author) for author in authors)  # ç¡®ä¿æ‰€æœ‰å…ƒç´ éƒ½æ˜¯å­—ç¬¦ä¸²

def get_daily_papers(query, max_results) -> dict[str, ArxivPaper]:
    paper_result = {}
    search_engine = arxiv.Search(
        query = query,
        max_results = max_results,
        sort_by = arxiv.SortCriterion.SubmittedDate
    )

    for result in search_engine.results():
        paper_id            = result.get_short_id()
        paper_title         = result.title
        paper_url           = result.entry_id
        paper_abstract      = result.summary.replace("\n"," ")
        paper_authors       = get_authors(result.authors)
        paper_first_author  = get_authors(result.authors, first_author = True)
        primary_category    = result.primary_category
        publish_time        = result.published.date()
        update_time         = result.updated.date()
        comments            = result.comment


        logging.info(f"Time = {update_time} title = {paper_title} author = {paper_first_author}")

        # eg: 2108.09112v1 -> 2108.09112
        ver_pos = paper_id.find('v')
        if ver_pos == -1:
            paper_key = paper_id
        else:
            paper_key = paper_id[0:ver_pos]
        paper_url = ARXIV_URL + 'abs/' + paper_key

        arxiv_paper = ArxivPaper(
          paper_id=paper_id,
          paper_title=paper_title,
          paper_url=paper_url,
          paper_abstract=paper_abstract,
          paper_authors=paper_authors,
          paper_first_author=paper_first_author,
          primary_category=primary_category,
          publish_time=publish_time,
          update_time=update_time,
          comments=comments
        )
        paper_result[paper_key] = arxiv_paper

    return paper_result

def save_to_parquet(papers: dict[str, ArxivPaper], meta_file: str):
    """ä¿å­˜è®ºæ–‡æ•°æ®åˆ°parquetæ–‡ä»¶ï¼ˆå¢åŠ pushedå­—æ®µï¼‰"""
    Path("data").mkdir(exist_ok=True)
    
    # è¯»å–å·²æœ‰æ•°æ®ï¼ˆå¦‚æœæ–‡ä»¶å­˜åœ¨ï¼‰
    existing_df = pd.DataFrame()
    if Path(meta_file).exists():
        try:
            existing_df = pd.read_parquet(meta_file)
        except Exception as e:
            logging.warning(f"Error reading existing file: {str(e)}")
    
    # åˆå¹¶æ–°æ—§æ•°æ®æ—¶æ·»åŠ pushedå­—æ®µ
    new_df = pd.DataFrame.from_dict(papers, orient='index')
    new_df['summary'] = None
    new_df['pushed'] = False  # æ–°å¢æ¨é€çŠ¶æ€å­—æ®µ
    combined_df = pd.concat([existing_df, new_df], ignore_index=False)
    
    # å»é‡ï¼ˆä¿ç•™æœ€åå‡ºç°çš„è®°å½•ï¼‰å¹¶ä¿å­˜
    combined_df = combined_df[~combined_df.index.duplicated(keep='last')]
    combined_df.to_parquet(meta_file, engine='pyarrow')

def send_to_feishu(paper: ArxivPaper, summary: str) -> bool:
    """å‘é€å•ç¯‡è®ºæ–‡åˆ°é£ä¹¦ï¼ˆè¿”å›æ˜¯å¦æˆåŠŸï¼‰"""
    if not FEISHU_WEBHOOK_URL:
        logging.error("é£ä¹¦Webhookåœ°å€æœªé…ç½®")
        return False

    formatted_summary = summary.replace("\\n", "\n")
    
    message = {
        "msg_type": "interactive",
        "card": {
            "elements": [{
                "tag": "div",
                "text": {
                    "content": f"**{paper['paper_title']}**\n"
                               f"**æ›´æ–°æ—¶é—´**: {paper['update_time']}\n\n"
                               f"ğŸ‘¤ {paper['paper_authors']}\n\n"
                               f"ğŸ’¡ AIæ€»ç»“ï¼š{formatted_summary}...\n\n"
                               f"---\n"
                               f"ğŸ“ [è®ºæ–‡åŸæ–‡]({paper['paper_url']})",
                    "tag": "lark_md"
                }
            }],
            "header": {
                "title": {
                    "content": "ğŸ“„ æ–°è®ºæ–‡æ¨è",
                    "tag": "plain_text"
                }
            }
        }
    }

    try:
        send_to_feishu_with_retry(message)
        logging.info(f"é£ä¹¦æ¨é€æˆåŠŸ: {paper['paper_id']}")
        return True
    except Exception as e:
        logging.error(f"é£ä¹¦æ¨é€å¤±è´¥: {str(e)}")
        return False

def push_to_feishu(df: pd.DataFrame, meta_file: str) -> pd.DataFrame:
    """æ‰¹é‡æ¨é€æœªå‘é€è®ºæ–‡å¹¶æ›´æ–°çŠ¶æ€"""
    # ç­›é€‰éœ€è¦æ¨é€çš„è®ºæ–‡
    to_push = df[(df['pushed'] == False) & 
                (df['summary'].notna())].copy()
    
    if to_push.empty:
        logging.info("æ²¡æœ‰éœ€è¦æ¨é€çš„æ–°è®ºæ–‡")
        return df
    
    # æŒ‰æ—¶é—´æ’åºï¼ˆæ—§åˆ°æ–°ï¼‰
    sorted_df = to_push.sort_values('update_time', ascending=True)
    
    # æ‰¹é‡å¤„ç†æ¨é€
    success_indices = []
    for index, row in sorted_df.iterrows():
        paper = ArxivPaper(
            paper_id=row['paper_id'],
            paper_title=row['paper_title'],
            paper_url=row['paper_url'],
            paper_abstract=row['paper_abstract'],
            paper_authors=row['paper_authors'],
            paper_first_author=row['paper_first_author'],
            primary_category=row['primary_category'],
            publish_time=row['publish_time'],
            update_time=row['update_time'],
            comments=row['comments']
        )
        if send_to_feishu(paper, row['summary']):
            success_indices.append(index)
        else:
            logging.error(f"é£ä¹¦æ¨é€å¤±è´¥: {paper['paper_id']} {paper['paper_title']}")
    
    # æ‰¹é‡æ›´æ–°æ¨é€çŠ¶æ€
    if success_indices:
        df.loc[success_indices, 'pushed'] = True
        df.to_parquet(meta_file, engine='pyarrow')
        logging.info(f"æˆåŠŸæ›´æ–°{len(success_indices)}ç¯‡è®ºæ–‡æ¨é€çŠ¶æ€")
    
    return df

def filter_existing_papers(new_papers: dict[str, ArxivPaper], meta_file: str) -> dict[str, ArxivPaper]:
    """è¿‡æ»¤å·²å­˜åœ¨çš„è®ºæ–‡ï¼ˆå‚æ•°åŒ–ç‰ˆæœ¬ï¼‰"""
    existing_ids = set()
    
    if Path(meta_file).exists():
        try:
            df = pd.read_parquet(meta_file)
            if not df.empty and 'paper_id' in df.columns:
                existing_ids.update(df['paper_id'].tolist())
        except Exception as e:
            logging.warning(f"Error reading {meta_file}: {str(e)}")
    
    return {k: v for k, v in new_papers.items() if v['paper_id'] not in existing_ids}

class PaperAnalysis(dspy.Signature):
    """åˆ†æè®ºæ–‡æ‘˜è¦å¹¶åˆ¤æ–­æ˜¯å¦å±äºæŸä¸€ä¸ªç‰¹å®šé¢†åŸŸ"""
    input_paper_text: str = dspy.InputField(desc="è®ºæ–‡çš„æ‘˜è¦")
    input_domain: str = dspy.InputField(desc="é¢†åŸŸå")
    output_domain: bool = dspy.OutputField(desc="æ˜¯å¦å±äºç”¨æˆ·ç»™å®šçš„é¢†åŸŸ, è¿”å›trueæˆ–false")

def analyze_paper(paper: ArxivPaper, domain: str) -> bool:
    """
    ä½¿ç”¨PaperAnalysisåˆ†æè®ºæ–‡æ˜¯å¦å±äºç‰¹å®šé¢†åŸŸã€‚
    
    :param paper: è¦åˆ†æçš„è®ºæ–‡ï¼Œç±»å‹ä¸ºArxivPaper
    :param domain: è¦åˆ¤æ–­çš„é¢†åŸŸå
    :return: å¦‚æœè®ºæ–‡å±äºè¯¥é¢†åŸŸè¿”å›Trueï¼Œå¦åˆ™è¿”å›False
    """
    analysis = PaperAnalysis()
    result = analysis(input_paper_text=paper['paper_abstract'], input_domain=domain)
    return result.output_domain

def summarize_paper(lm, paper_text) -> str:
    # ä¿®æ­£å†’å·ä¸ºè‹±æ–‡æ ¼å¼ï¼Œä½¿ç”¨æ ‡å‡†ç­¾åè¯­æ³•
    prompt = f"ç”¨ä¸­æ–‡å¸®æˆ‘ä»‹ç»ä¸€ä¸‹è¿™ç¯‡æ–‡ç« : {paper_text}"
    summary = lm(prompt)
    return summary[0]

def extract_text_from_pdf(pdf_path):
    """æå–PDFæ–‡æœ¬å†…å®¹ï¼ˆå¢åŠ åŒè§£æå¼•æ“ï¼‰"""
    try:
        # å°è¯•ä½¿ç”¨PyPDF2è§£æ
        with open(pdf_path, 'rb') as f:
            reader = PdfReader(f)
            text = '\n'.join([page.extract_text() for page in reader.pages])
            # æ–°å¢Unicodeæ¸…ç†
            return text.encode('utf-8', 'ignore').decode('utf-8')  # è¿‡æ»¤æ— æ•ˆå­—ç¬¦
    except Exception as pdf_error:
        print(f"PyPDF2è§£æå¤±è´¥ï¼Œå°è¯•å¤‡ç”¨è§£æå¼•æ“: {pdf_path}")
        try:
            # å¤‡é€‰æ–¹æ¡ˆ1ï¼šä½¿ç”¨pdfplumberï¼ˆéœ€è¦å®‰è£…ï¼‰
            import pdfplumber
            with pdfplumber.open(pdf_path) as pdf:
                text = '\n'.join([page.extract_text() for page in pdf.pages])
                return text.encode('utf-8', 'ignore').decode('utf-8')  # è¿‡æ»¤æ— æ•ˆå­—ç¬¦
        except Exception as plumber_error:
            try:
                # å¤‡é€‰æ–¹æ¡ˆ2ï¼šä½¿ç”¨PyMuPDFï¼ˆéœ€è¦å®‰è£…ï¼‰
                import fitz  # PyMuPDFçš„å¯¼å…¥åç§°
                doc = fitz.open(pdf_path)
                text = '\n'.join([page.get_text() for page in doc])
                return text.encode('utf-8', 'ignore').decode('utf-8')  # è¿‡æ»¤æ— æ•ˆå­—ç¬¦
            except Exception as fitz_error:
                error_msg = (
                    f"PDFè§£æå…¨éƒ¨å¤±è´¥: {pdf_path}\n"
                    f"PyPDF2é”™è¯¯: {str(pdf_error)}\n"
                    f"pdfplumberé”™è¯¯: {str(plumber_error)}\n"
                    f"PyMuPDFé”™è¯¯: {str(fitz_error)}"
                )
                print(error_msg)
                return ""

def download_paper(url: str, paper_id: str, save_dir: str, retries=3):
    """ä¸‹è½½å¹¶ä¿å­˜PDFè®ºæ–‡ï¼ˆå¢åŠ é‡è¯•æœºåˆ¶ï¼‰"""
    os.makedirs(save_dir, exist_ok=True)
    file_path = os.path.join(save_dir, f"{paper_id}.pdf")
    
    if os.path.exists(file_path):
        print(f"æ–‡ä»¶å·²å­˜åœ¨ï¼Œè·³è¿‡ä¸‹è½½: {paper_id}")
        return
    
    for attempt in range(retries):
        try:
            response = requests.get(url, stream=True, timeout=30)
            response.raise_for_status()
            
            # å¢åŠ æ–‡ä»¶å®Œæ•´æ€§æ ¡éªŒ
            total_size = int(response.headers.get('content-length', 0))
            downloaded = 0
            
            with open(file_path, 'wb') as f:
                for chunk in response.iter_content(chunk_size=8192):
                    downloaded += len(chunk)
                    f.write(chunk)
                    
            # ç®€å•æ ¡éªŒæ–‡ä»¶å®Œæ•´æ€§
            if total_size > 0 and downloaded != total_size:
                raise IOError("æ–‡ä»¶å¤§å°ä¸åŒ¹é…ï¼Œå¯èƒ½ä¸‹è½½ä¸å®Œæ•´")
                
            print(f"æˆåŠŸä¸‹è½½: {paper_id}")
            return
        except Exception as e:
            if attempt < retries - 1:
                print(f"ä¸‹è½½å¤±è´¥ {paper_id}ï¼Œç¬¬{attempt+1}æ¬¡é‡è¯•...")
                time.sleep(2)
            else:
                print(f"ä¸‹è½½æœ€ç»ˆå¤±è´¥ {paper_id}: {str(e)}")
                try:
                    os.remove(file_path)
                except:
                    pass

async def process_single_paper(executor, lm, paper, row_index):
    """å¹¶å‘å¤„ç†å•ç¯‡è®ºæ–‡çš„å¼‚æ­¥ä»»åŠ¡"""
    loop = asyncio.get_event_loop()
    
    # ä¸‹è½½è®ºæ–‡ï¼ˆä½¿ç”¨çº¿ç¨‹æ± æ‰§è¡Œé˜»å¡IOï¼‰
    pdf_url = paper['paper_url'].replace('abs', 'pdf')
    await loop.run_in_executor(executor, download_paper, pdf_url, paper['paper_id'], 'papers')
    
    # æå–æ–‡æœ¬
    pdf_path = os.path.join('papers', f"{paper['paper_id']}.pdf")
    paper_text = await loop.run_in_executor(executor, extract_text_from_pdf, pdf_path)
    
    # æ–°å¢æ–‡æœ¬æˆªæ–­é€»è¾‘
    truncated_text = paper_text
    if len(paper_text) > MAX_PAPER_TEXT_LENGTH:
        print(f"è®ºæ–‡æˆªæ–­è­¦å‘Š: {paper['paper_title']}ï¼ˆID: {paper['paper_id']}ï¼‰æ–‡æœ¬é•¿åº¦ {len(paper_text)} å­—ç¬¦ï¼Œå·²æˆªæ–­")
        truncated_text = paper_text[:MAX_PAPER_TEXT_LENGTH] + "[...æˆªæ–­...]"
    
    # æ€»ç»“è®ºæ–‡ï¼ˆä½¿ç”¨æˆªæ–­åçš„æ–‡æœ¬ï¼‰
    summary = await loop.run_in_executor(executor, summarize_paper, lm, truncated_text)
    
    return row_index, summary

from tenacity import retry, wait_exponential, stop_after_attempt

@retry(stop=stop_after_attempt(100), wait=wait_exponential(multiplier=1, min=1, max=10))
def send_to_feishu_with_retry(message):
    """å¸¦é‡è¯•æœºåˆ¶çš„é£ä¹¦æ¶ˆæ¯æ¨é€"""
    response = requests.post(
        FEISHU_WEBHOOK_URL,
        json=message,
        timeout=10
    )
    response.raise_for_status()

def generate_daily_summary(lm, df: pd.DataFrame, target_date: datetime.date = None) -> str:
    """ç”ŸæˆæŒ‡å®šæ—¥æœŸçš„ç®€æŠ¥å¹¶æ¨è3ç¯‡è®ºæ–‡"""
    # é»˜è®¤ä½¿ç”¨å½“å¤©æ—¥æœŸ
    target_date = target_date or datetime.date.today()
    
    # ç­›é€‰ç›®æ ‡æ—¥æœŸæ¨é€çš„è®ºæ–‡
    daily_papers = df[(df['update_time'] == target_date)]

    if (len(daily_papers) == 0):
        return None
    
    # æ„å»ºæ±‡æ€»æ–‡æœ¬
    combined_text = "ä»Šæ—¥è®ºæ–‡æ±‡æ€»ï¼š\n\n"
    for counter, (idx, row) in enumerate(daily_papers.iterrows(), 1):  # æ”¹ç”¨enumerateç”Ÿæˆåºå·
        combined_text += f"ã€è®ºæ–‡{counter}ã€‘{row['paper_title']}\nAIæ€»ç»“ï¼š{row['summary']}...\n\n"
    
    # LLMç”Ÿæˆç®€æŠ¥
    prompt = (
        f"è¯·å°†ä»¥ä¸‹è®ºæ–‡æ±‡æ€»ä¿¡æ¯æ•´ç†æˆä¸€ä»½ç»“æ„æ¸…æ™°çš„æ¯æ—¥ç®€æŠ¥ï¼ˆä½¿ç”¨ä¸­æ–‡ï¼‰ï¼š\n{combined_text}\n"
        "è¦æ±‚ï¼š\n1. åˆ†é¢†åŸŸæ€»ç»“ç ”ç©¶è¶‹åŠ¿\n2. ç”¨ç®€æ´çš„bullet pointså‘ˆç°\n3. æ¨è3ç¯‡æœ€å€¼å¾—é˜…è¯»çš„è®ºæ–‡å¹¶è¯´æ˜ç†ç”±\n4. é¢†åŸŸç›¸å…³è¶‹åŠ¿åˆ—å‡ºç›¸å…³è®ºæ–‡æ ‡é¢˜\n5. è®ºæ–‡æ ‡é¢˜ç”¨è‹±æ–‡è¡¨è¾¾\n"
        "6.åªè¾“å‡ºåˆ†é¢†åŸŸç ”ç©¶è¶‹åŠ¿æ€»ç»“å’Œæ¨èé˜…è¯»è®ºæ–‡ï¼Œä¸éœ€è¦è¾“å‡ºå…¶ä»–å†…å®¹\n7.è®ºæ–‡æ ‡é¢˜è¾“å‡ºæ—¶ä¸è¦çœç•¥"
    )
    return lm(prompt)[0]

def push_daily_summary(lm, df: pd.DataFrame, target_date: datetime.date = None):
    """æ¨é€æŒ‡å®šæ—¥æœŸçš„æ€»ç»“æŠ¥å‘Š"""
    daily_report = generate_daily_summary(lm, df, target_date)
    if daily_report == None:
      print(f"{target_date} æ²¡æœ‰éœ€è¦æ¨é€çš„æ—¥æŠ¥")
      return

    print(f"\n=== {target_date or 'æ¯æ—¥'}ç®€æŠ¥ ===")
    print(daily_report)
    
    if FEISHU_WEBHOOK_URL:
        target_date_display = target_date or datetime.date.today()
        message = {
            "msg_type": "interactive",
            "card": {
                "elements": [{
                    "tag": "div",
                    "text": {
                        "content": f"ğŸ“… AIè®ºæ–‡ç®€æŠ¥({target_date_display}){daily_report}",
                        "tag": "lark_md"
                    }
                }],
                "header": {
                    "title": {
                        "content": f"{target_date_display} è®ºæ–‡æ—¥æŠ¥",
                        "tag": "plain_text"
                    }
                }
            }
        }
        send_to_feishu_with_retry(message)

def process_papers_and_generate_summaries(lm, df: pd.DataFrame) -> pd.DataFrame:
    """å¤„ç†è®ºæ–‡ä¸‹è½½å¹¶ç”Ÿæˆæ‘˜è¦ï¼ˆè¿”å›æ›´æ–°åçš„DataFrameï¼‰"""
    # æ·»åŠ ç¼ºå¤±çš„summaryåˆ—ï¼ˆå…¼å®¹æ—§æ•°æ®ï¼‰
    if 'summary' not in df.columns:
        df['summary'] = None

    # è¿‡æ»¤æ‰å·²ç»æœ‰summaryå­—æ®µçš„è®ºæ–‡
    papers_without_summary = df[df['summary'].isna()]

    print(f"éœ€è¦å¤„ç†{len(papers_without_summary)}ç¯‡æ–°è®ºæ–‡")

    # åˆ›å»ºçº¿ç¨‹æ± æ‰§è¡Œå™¨
    executor = ThreadPoolExecutor(max_workers=20)
    # ä¿®å¤äº‹ä»¶å¾ªç¯åˆ›å»ºæ–¹å¼
    loop = asyncio.new_event_loop()
    asyncio.set_event_loop(loop)
    
    # å‡†å¤‡æ‰€æœ‰ä»»åŠ¡
    tasks = []
    for index, row in papers_without_summary.iterrows():
        paper = ArxivPaper(
            paper_id=row['paper_id'],
            paper_title=row['paper_title'],
            paper_url=row['paper_url'],
            paper_abstract=row['paper_abstract'],
            paper_authors=row['paper_authors'],
            paper_first_author=row['paper_first_author'],
            primary_category=row['primary_category'],
            publish_time=row['publish_time'],
            update_time=row['update_time'],
            comments=row['comments']
        )
        tasks.append(process_single_paper(executor, lm, paper, index))
    
    # ä½¿ç”¨tqdmæ˜¾ç¤ºå¹¶å‘ä»»åŠ¡è¿›åº¦
    from tqdm.asyncio import tqdm_asyncio
    results = loop.run_until_complete(
        tqdm_asyncio.gather(*tasks, desc="å¹¶å‘å¤„ç†è®ºæ–‡", total=len(tasks))
    )
    
    # æ‰¹é‡æ›´æ–°ç»“æœ
    for index, summary in results:
        df.at[index, 'summary'] = summary

    return df

def generate_weekly_summary_if_sunday(lm, df):
    """å¦‚æœæ˜¯å‘¨æ—¥åˆ™ç”Ÿæˆå‘¨æŠ¥ï¼Œå¦åˆ™ç”Ÿæˆæ—¥æŠ¥"""
    today = datetime.date.today()
    
    if today.weekday() == 6:  # å‘¨æ—¥ï¼ˆ0=å‘¨ä¸€ï¼Œ6=å‘¨æ—¥ï¼‰
        print("æ£€æµ‹åˆ°å‘¨æ—¥ï¼Œç”Ÿæˆæœ¬å‘¨æ‰€æœ‰æ—¥æŠ¥")
        # éå†è¿‡å»ä¸€å‘¨ï¼ˆå‘¨ä¸€åˆ°å‘¨æ—¥ï¼‰
        for i in range(6, -1, -1):
            past_day = today - datetime.timedelta(days=i)
            push_daily_summary(lm, df, past_day)
    else:
        print("ç”Ÿæˆä»Šæ—¥æ—¥æŠ¥")
        push_daily_summary(lm, df, today)

# ä¸»æµç¨‹ä¿®æ”¹
def main(query: str, 
        max_results: int,
        meta_file: str,
        lm: dspy.LM):
    """ä¸»æµç¨‹å‡½æ•°ï¼ˆå‚æ•°åŒ–ç‰ˆæœ¬ï¼‰"""

    # è·å–ä»Šæ—¥è®ºæ–‡
    new_papers = get_daily_papers(query, max_results)

    # è¿‡æ»¤å·²å­˜åœ¨è®ºæ–‡
    filtered_papers = filter_existing_papers(new_papers, meta_file)

    save_to_parquet(filtered_papers, meta_file)
    print(f"ä¿å­˜äº†{len(filtered_papers)}ç¯‡æ–°è®ºæ–‡")
    
    # è¯»å–ä¿å­˜çš„è®ºæ–‡æ•°æ®
    df = pd.read_parquet(meta_file)

    # å¤„ç†è®ºæ–‡å¹¶ç”Ÿæˆæ‘˜è¦
    df = process_papers_and_generate_summaries(lm, df)

    # TODO(ysj): filter paper by user specified summary
    
    # ä¿å­˜æ›´æ–°åçš„DataFrame
    df.to_parquet(meta_file, engine='pyarrow')
    
    # df = reset_recent_pushed_status(df, 7)
    
    push_to_feishu(df, meta_file)
    
    # generate_weekly_summary_if_sunday(lm, df)


def reset_recent_pushed_status(df: pd.DataFrame, days: int, meta_file: str) -> pd.DataFrame:
    """é‡ç½®æ¨é€çŠ¶æ€ï¼ˆå‚æ•°åŒ–ç‰ˆæœ¬ï¼‰"""
    # è®¡ç®—æ—¥æœŸèŒƒå›´
    cutoff_date = datetime.date.today() - datetime.timedelta(days=days)
    
    # ç­›é€‰éœ€è¦é‡ç½®çš„è®°å½•ï¼ˆä½¿ç”¨locé¿å…é“¾å¼èµ‹å€¼è­¦å‘Šï¼‰
    mask = df['update_time'] >= cutoff_date
    reset_count = df.loc[mask, 'pushed'].sum()
    
    # æ‰§è¡ŒçŠ¶æ€é‡ç½®
    df.loc[mask, 'pushed'] = FalsWithSummarye
    
    # ä¿å­˜æ›´æ–°åˆ°æ–‡ä»¶
    df.to_parquet(meta_file, engine='pyarrow')
    logging.info(f"å·²é‡ç½®æœ€è¿‘{days}å¤©å†…{reset_count}ç¯‡è®ºæ–‡çš„æ¨é€çŠ¶æ€")
    return df

def rag_papers(lm):
    main("\"RAG\" OR \"Retrieval-Augmented Generation\"", 40, "data/daily_papers.parquet", lm)

def kg_papers(lm):
    main("\"knowledge-graph\" OR \"knowledge graph\"", 40, "data/daily_papers_kg.parquet", lm)

if __name__ == "__main__":
    # é…ç½®dspy
    lm = dspy.LM("openai/" + CHAT_MODEL_NAME, api_base=LLM_BASE_URL, api_key=LLM_API_KEY, temperature=0.2)
    dspy.configure(lm=lm)

    parser = argparse.ArgumentParser()
    parser.add_argument("--task", type=str, default="", help="ä»»åŠ¡åç§°")
    args = parser.parse_args()

    if args.task == "rag":
        rag_papers(lm)
    elif args.task == "kg":
        kg_papers(lm)
    else:
        print("æœªçŸ¥ä»»åŠ¡")
